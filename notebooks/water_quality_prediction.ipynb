{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d65215ee",
   "metadata": {},
   "source": [
    "# Water Quality Prediction with TensorFlow\n",
    "\n",
    "## Professional Machine Learning Project\n",
    "\n",
    "This notebook demonstrates a complete machine learning pipeline for predicting water quality for consumption based on three key sensor measurements:\n",
    "\n",
    "- **TDS (Total Dissolved Solids)**: Measures dissolved inorganic and organic substances (mg/L)\n",
    "- **Turbidity**: Measures water clarity/cloudiness (NTU - Nephelometric Turbidity Units)\n",
    "- **pH**: Measures acidity/alkalinity levels (0-14 scale)\n",
    "\n",
    "### Project Objectives\n",
    "1. Generate realistic synthetic water quality data\n",
    "2. Build a TensorFlow neural network for classification\n",
    "3. Predict water quality categories: Poor, Acceptable, Good, Excellent\n",
    "4. Provide actionable insights for water consumption safety"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16d5471",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import all necessary libraries for machine learning, data analysis, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b86089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Scikit-learn utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bbf123",
   "metadata": {},
   "source": [
    "## 2. Generate Synthetic Water Quality Dataset\n",
    "\n",
    "Create a realistic synthetic dataset based on WHO and EPA water quality standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f720f420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Water Quality Standards\n",
    "QUALITY_LABELS = {\n",
    "    0: 'Poor',\n",
    "    1: 'Acceptable',\n",
    "    2: 'Good', \n",
    "    3: 'Excellent'\n",
    "}\n",
    "\n",
    "def determine_quality_score(tds, turbidity, ph):\n",
    "    \"\"\"Determine water quality based on sensor readings\"\"\"\n",
    "    # pH scoring (7.0-7.5 is optimal)\n",
    "    if 7.0 <= ph <= 7.5:\n",
    "        ph_score = 3\n",
    "    elif 6.5 <= ph <= 8.5:\n",
    "        ph_score = 2\n",
    "    elif 6.0 <= ph <= 9.0:\n",
    "        ph_score = 1\n",
    "    else:\n",
    "        ph_score = 0\n",
    "    \n",
    "    # TDS scoring (lower is better)\n",
    "    if tds <= 300:\n",
    "        tds_score = 3\n",
    "    elif tds <= 600:\n",
    "        tds_score = 2\n",
    "    elif tds <= 900:\n",
    "        tds_score = 1\n",
    "    else:\n",
    "        tds_score = 0\n",
    "    \n",
    "    # Turbidity scoring (lower is better)\n",
    "    if turbidity <= 1:\n",
    "        turbidity_score = 3\n",
    "    elif turbidity <= 4:\n",
    "        turbidity_score = 2\n",
    "    elif turbidity <= 10:\n",
    "        turbidity_score = 1\n",
    "    else:\n",
    "        turbidity_score = 0\n",
    "    \n",
    "    # Weighted average (pH is most critical)\n",
    "    weights = [0.4, 0.3, 0.3]  # pH, TDS, Turbidity\n",
    "    scores = [ph_score, tds_score, turbidity_score]\n",
    "    weighted_score = np.average(scores, weights=weights)\n",
    "    \n",
    "    # Convert to discrete quality levels\n",
    "    if weighted_score >= 2.5:\n",
    "        return 3  # Excellent\n",
    "    elif weighted_score >= 1.5:\n",
    "        return 2  # Good\n",
    "    elif weighted_score >= 0.5:\n",
    "        return 1  # Acceptable\n",
    "    else:\n",
    "        return 0  # Poor\n",
    "\n",
    "def generate_realistic_sample(quality_target=None):\n",
    "    \"\"\"Generate a single realistic water sample\"\"\"\n",
    "    if quality_target is None:\n",
    "        quality_target = np.random.choice([0, 1, 2, 3], p=[0.15, 0.25, 0.35, 0.25])\n",
    "    \n",
    "    if quality_target == 3:  # Excellent\n",
    "        ph = np.random.normal(7.25, 0.15)\n",
    "        tds = np.random.normal(200, 50)\n",
    "        turbidity = np.random.exponential(0.5)\n",
    "    elif quality_target == 2:  # Good\n",
    "        ph = np.random.normal(7.0, 0.4)\n",
    "        tds = np.random.normal(450, 100)\n",
    "        turbidity = np.random.exponential(2.0)\n",
    "    elif quality_target == 1:  # Acceptable\n",
    "        ph = np.random.normal(6.8, 0.8)\n",
    "        tds = np.random.normal(750, 150)\n",
    "        turbidity = np.random.exponential(6.0)\n",
    "    else:  # Poor\n",
    "        ph = np.random.choice([\n",
    "            np.random.normal(5.5, 0.5),  # Too acidic\n",
    "            np.random.normal(9.5, 0.5)   # Too alkaline\n",
    "        ])\n",
    "        tds = np.random.normal(1200, 300)\n",
    "        turbidity = np.random.exponential(15.0)\n",
    "    \n",
    "    # Apply realistic bounds\n",
    "    ph = np.clip(ph, 4.0, 12.0)\n",
    "    tds = np.clip(tds, 50, 3000)\n",
    "    turbidity = np.clip(turbidity, 0.1, 50)\n",
    "    \n",
    "    return tds, turbidity, ph\n",
    "\n",
    "# Generate dataset\n",
    "print(\"Generating water quality dataset...\")\n",
    "n_samples = 8000\n",
    "data = []\n",
    "\n",
    "# Generate samples for each quality level\n",
    "quality_distribution = [0.15, 0.25, 0.35, 0.25]  # Poor, Acceptable, Good, Excellent\n",
    "\n",
    "for quality in range(4):\n",
    "    n_samples_quality = int(n_samples * quality_distribution[quality])\n",
    "    \n",
    "    for _ in range(n_samples_quality):\n",
    "        tds, turbidity, ph = generate_realistic_sample(quality)\n",
    "        \n",
    "        # Verify quality matches expectations (with some noise)\n",
    "        actual_quality = determine_quality_score(tds, turbidity, ph)\n",
    "        \n",
    "        # Add some noise to make it more realistic\n",
    "        if np.random.random() < 0.1:  # 10% noise\n",
    "            actual_quality = np.random.choice([max(0, actual_quality-1), \n",
    "                                             min(3, actual_quality+1)])\n",
    "        \n",
    "        data.append({\n",
    "            'tds': round(tds, 2),\n",
    "            'turbidity': round(turbidity, 2),\n",
    "            'ph': round(ph, 2),\n",
    "            'quality': actual_quality,\n",
    "            'quality_label': QUALITY_LABELS[actual_quality]\n",
    "        })\n",
    "\n",
    "# Shuffle and create DataFrame\n",
    "np.random.shuffle(data)\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(f\"Dataset created with {len(df)} samples\")\n",
    "print(f\"Quality distribution:\\n{df['quality_label'].value_counts()}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f04f0b1",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing and Exploration\n",
    "\n",
    "Explore the dataset structure, visualize distributions, and analyze relationships between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6468252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset overview\n",
    "print(\"=== Dataset Information ===\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nData types:\\n{df.dtypes}\")\n",
    "print(f\"\\nMissing values:\\n{df.isnull().sum()}\")\n",
    "print(f\"\\nBasic statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecf6935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature distributions by quality\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "features = ['tds', 'turbidity', 'ph']\n",
    "colors = ['red', 'orange', 'lightblue', 'green']\n",
    "\n",
    "# Feature distributions\n",
    "for i, feature in enumerate(features):\n",
    "    ax = axes[i//2, i%2]\n",
    "    \n",
    "    for j, quality in enumerate(['Poor', 'Acceptable', 'Good', 'Excellent']):\n",
    "        subset = df[df['quality_label'] == quality]\n",
    "        ax.hist(subset[feature], alpha=0.7, label=quality, bins=30, color=colors[j])\n",
    "    \n",
    "    ax.set_xlabel(feature.upper())\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title(f'{feature.upper()} Distribution by Quality')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Quality distribution pie chart\n",
    "ax = axes[1, 1]\n",
    "quality_counts = df['quality_label'].value_counts()\n",
    "ax.pie(quality_counts.values, labels=quality_counts.index, autopct='%1.1f%%', \n",
    "       colors=colors, startangle=90)\n",
    "ax.set_title('Overall Quality Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d50566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Calculate correlation matrix\n",
    "numeric_df = df[['tds', 'turbidity', 'ph', 'quality']].copy()\n",
    "correlation_matrix = numeric_df.corr()\n",
    "\n",
    "# Plot heatmap\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, fmt='.3f', cbar_kws={'label': 'Correlation Coefficient'})\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Correlation insights:\")\n",
    "for feature in ['tds', 'turbidity', 'ph']:\n",
    "    corr = correlation_matrix.loc[feature, 'quality']\n",
    "    print(f\"- {feature.upper()} correlation with quality: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2aa2bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pairwise feature relationships\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Create pairplot\n",
    "plot_df = df[['tds', 'turbidity', 'ph', 'quality_label']].copy()\n",
    "g = sns.pairplot(plot_df, hue='quality_label', diag_kind='hist',\n",
    "                 plot_kws={'alpha': 0.6}, diag_kws={'alpha': 0.7},\n",
    "                 palette=['red', 'orange', 'lightblue', 'green'])\n",
    "\n",
    "g.fig.suptitle('Feature Relationships by Water Quality', y=1.02, fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea7f1e8",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering and Scaling\n",
    "\n",
    "Prepare the data for machine learning by scaling features and splitting into train/validation/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44a2aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "feature_columns = ['tds', 'turbidity', 'ph']\n",
    "X = df[feature_columns].copy()\n",
    "y = df['quality'].copy()\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nTarget distribution:\\n{pd.Series(y).value_counts().sort_index()}\")\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"\\nData split:\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(df)*100:.1f}%)\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples ({X_val.shape[0]/len(df)*100:.1f}%)\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbb3741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit scaler on training data and transform all sets\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert labels to categorical for neural network\n",
    "y_train_cat = to_categorical(y_train, num_classes=4)\n",
    "y_val_cat = to_categorical(y_val, num_classes=4)\n",
    "y_test_cat = to_categorical(y_test, num_classes=4)\n",
    "\n",
    "print(\"Feature scaling completed\")\n",
    "print(f\"Scaled training features shape: {X_train_scaled.shape}\")\n",
    "print(f\"Categorical training labels shape: {y_train_cat.shape}\")\n",
    "\n",
    "# Calculate class weights for imbalanced dataset\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weight_dict = dict(enumerate(class_weights))\n",
    "print(f\"\\nClass weights: {class_weight_dict}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4c0f20",
   "metadata": {},
   "source": [
    "## 5. Build TensorFlow Neural Network Model\n",
    "\n",
    "Design and compile a neural network architecture optimized for water quality classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44c9b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_water_quality_model(input_dim=3, num_classes=4):\n",
    "    \"\"\"Build neural network for water quality classification\"\"\"\n",
    "    \n",
    "    model = Sequential([\n",
    "        # Input layer with L2 regularization\n",
    "        Dense(64, input_dim=input_dim, activation='relu', \n",
    "              kernel_regularizer=l2(0.001), name='input_layer'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Hidden layer 1\n",
    "        Dense(32, activation='relu', kernel_regularizer=l2(0.001), name='hidden_1'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Hidden layer 2\n",
    "        Dense(16, activation='relu', kernel_regularizer=l2(0.001), name='hidden_2'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Output layer\n",
    "        Dense(num_classes, activation='softmax', name='output_layer')\n",
    "    ])\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "model = build_water_quality_model(input_dim=X_train_scaled.shape[1])\n",
    "\n",
    "# Display model architecture\n",
    "print(\"Neural Network Architecture:\")\n",
    "model.summary()\n",
    "\n",
    "# Visualize model architecture\n",
    "tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True, dpi=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2d42c6",
   "metadata": {},
   "source": [
    "## 6. Train the Model\n",
    "\n",
    "Train the neural network with callbacks for optimal performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fce40e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=5,\n",
    "        min_lr=0.0001,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Starting model training...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train_cat,\n",
    "    validation_data=(X_val_scaled, y_val_cat),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6015d95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot loss\n",
    "ax1.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "ax1.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "ax1.set_title('Model Loss During Training', fontsize=14)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot accuracy\n",
    "ax2.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)\n",
    "ax2.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "ax2.set_title('Model Accuracy During Training', fontsize=14)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final metrics\n",
    "final_train_acc = history.history['accuracy'][-1]\n",
    "final_val_acc = history.history['val_accuracy'][-1]\n",
    "final_train_loss = history.history['loss'][-1]\n",
    "final_val_loss = history.history['val_loss'][-1]\n",
    "\n",
    "print(f\"\\nFinal Training Metrics:\")\n",
    "print(f\"Training Accuracy: {final_train_acc:.4f}\")\n",
    "print(f\"Validation Accuracy: {final_val_acc:.4f}\")\n",
    "print(f\"Training Loss: {final_train_loss:.4f}\")\n",
    "print(f\"Validation Loss: {final_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285b3ad1",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation and Metrics\n",
    "\n",
    "Evaluate model performance on the test set and analyze classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292ffe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "y_test_pred_proba = model.predict(X_test_scaled)\n",
    "y_test_pred = np.argmax(y_test_pred_proba, axis=1)\n",
    "\n",
    "# Calculate test accuracy\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\n=== Classification Report ===\")\n",
    "report = classification_report(\n",
    "    y_test, y_test_pred,\n",
    "    target_names=[QUALITY_LABELS[i] for i in range(4)],\n",
    "    digits=4\n",
    ")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa41fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix visualization\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=[QUALITY_LABELS[i] for i in range(4)],\n",
    "            yticklabels=[QUALITY_LABELS[i] for i in range(4)],\n",
    "            cbar_kws={'label': 'Number of Samples'})\n",
    "plt.title('Confusion Matrix - Water Quality Prediction', fontsize=16)\n",
    "plt.xlabel('Predicted Quality', fontsize=12)\n",
    "plt.ylabel('Actual Quality', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate per-class accuracy\n",
    "print(\"\\nPer-class accuracy:\")\n",
    "for i in range(4):\n",
    "    class_accuracy = cm[i, i] / cm[i, :].sum()\n",
    "    print(f\"{QUALITY_LABELS[i]}: {class_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bffacd5",
   "metadata": {},
   "source": [
    "## 8. Real-time Prediction Function\n",
    "\n",
    "Create a function for making real-time predictions on new sensor readings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272b6a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_water_quality(tds, turbidity, ph, model, scaler):\n",
    "    \"\"\"\n",
    "    Predict water quality for given sensor readings\n",
    "    \n",
    "    Args:\n",
    "        tds (float): Total Dissolved Solids (mg/L)\n",
    "        turbidity (float): Turbidity (NTU)\n",
    "        ph (float): pH level\n",
    "        model: Trained TensorFlow model\n",
    "        scaler: Fitted StandardScaler\n",
    "    \n",
    "    Returns:\n",
    "        dict: Prediction results\n",
    "    \"\"\"\n",
    "    # Validate inputs\n",
    "    if not (4.0 <= ph <= 12.0):\n",
    "        return {\"error\": f\"pH {ph} is outside valid range (4.0-12.0)\"}\n",
    "    \n",
    "    if not (0 <= tds <= 5000):\n",
    "        return {\"error\": f\"TDS {tds} is outside valid range (0-5000 mg/L)\"}\n",
    "        \n",
    "    if not (0 <= turbidity <= 100):\n",
    "        return {\"error\": f\"Turbidity {turbidity} is outside valid range (0-100 NTU)\"}\n",
    "    \n",
    "    # Prepare input\n",
    "    sample = np.array([[tds, turbidity, ph]])\n",
    "    sample_scaled = scaler.transform(sample)\n",
    "    \n",
    "    # Make prediction\n",
    "    pred_proba = model.predict(sample_scaled, verbose=0)\n",
    "    pred_class = np.argmax(pred_proba, axis=1)[0]\n",
    "    confidence = pred_proba[0][pred_class]\n",
    "    \n",
    "    # Generate recommendation\n",
    "    if confidence < 0.7:\n",
    "        recommendation = \"Low confidence prediction. Consider additional testing.\"\n",
    "    elif pred_class == 3:\n",
    "        recommendation = \"✅ Excellent water quality. Safe for consumption.\"\n",
    "    elif pred_class == 2:\n",
    "        recommendation = \"✅ Good water quality. Generally safe for consumption.\"\n",
    "    elif pred_class == 1:\n",
    "        recommendation = \"⚠️ Acceptable water quality. Monitor regularly.\"\n",
    "    else:\n",
    "        recommendation = \"❌ Poor water quality. Treatment required before consumption.\"\n",
    "    \n",
    "    return {\n",
    "        'quality_class': int(pred_class),\n",
    "        'quality_label': QUALITY_LABELS[pred_class],\n",
    "        'confidence': float(confidence),\n",
    "        'probabilities': {QUALITY_LABELS[i]: float(pred_proba[0][i]) for i in range(4)},\n",
    "        'recommendation': recommendation\n",
    "    }\n",
    "\n",
    "# Test the prediction function\n",
    "print(\"=== Testing Prediction Function ===\")\n",
    "\n",
    "test_cases = [\n",
    "    (250, 0.8, 7.2, \"Excellent quality water\"),\n",
    "    (450, 2.5, 7.0, \"Good quality water\"),\n",
    "    (800, 5.0, 6.8, \"Acceptable water\"),\n",
    "    (1500, 15.0, 5.5, \"Poor quality water\")\n",
    "]\n",
    "\n",
    "for tds, turbidity, ph, description in test_cases:\n",
    "    result = predict_water_quality(tds, turbidity, ph, model, scaler)\n",
    "    \n",
    "    print(f\"\\n{description}:\")\n",
    "    print(f\"Input: TDS={tds}, Turbidity={turbidity}, pH={ph}\")\n",
    "    \n",
    "    if \"error\" not in result:\n",
    "        print(f\"Predicted: {result['quality_label']} (Confidence: {result['confidence']:.1%})\")\n",
    "        print(f\"Recommendation: {result['recommendation']}\")\n",
    "    else:\n",
    "        print(f\"Error: {result['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bbbaa6",
   "metadata": {},
   "source": [
    "## 9. Visualize Results and Feature Importance\n",
    "\n",
    "Create comprehensive visualizations to understand model behavior and feature relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2b9b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction confidence analysis\n",
    "all_pred_proba = model.predict(X_test_scaled)\n",
    "all_pred_classes = np.argmax(all_pred_proba, axis=1)\n",
    "confidence_scores = np.max(all_pred_proba, axis=1)\n",
    "\n",
    "# Plot confidence distribution by quality\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i, quality in enumerate(['Poor', 'Acceptable', 'Good', 'Excellent']):\n",
    "    mask = all_pred_classes == i\n",
    "    confidences = confidence_scores[mask]\n",
    "    \n",
    "    plt.hist(confidences, alpha=0.7, label=f'{quality} (n={len(confidences)})', \n",
    "             bins=20, color=colors[i])\n",
    "\n",
    "plt.xlabel('Prediction Confidence')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Prediction Confidence Distribution by Quality Class')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average confidence: {np.mean(confidence_scores):.4f}\")\n",
    "print(f\"Predictions with >90% confidence: {np.sum(confidence_scores > 0.9)/len(confidence_scores)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277da5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature impact visualization\n",
    "# Create a grid to show how each feature affects predictions\n",
    "\n",
    "def visualize_feature_impact(feature_name, feature_idx, model, scaler):\n",
    "    \"\"\"Visualize how a single feature affects predictions\"\"\"\n",
    "    \n",
    "    # Set default values (typical good water)\n",
    "    default_values = [300, 2.0, 7.0]  # TDS, Turbidity, pH\n",
    "    \n",
    "    # Define feature ranges\n",
    "    feature_ranges = {\n",
    "        'tds': np.linspace(100, 1500, 100),\n",
    "        'turbidity': np.linspace(0.1, 20, 100),\n",
    "        'ph': np.linspace(5.0, 9.0, 100)\n",
    "    }\n",
    "    \n",
    "    feature_range = feature_ranges[feature_name]\n",
    "    predictions = []\n",
    "    \n",
    "    for value in feature_range:\n",
    "        # Create sample with varying feature\n",
    "        sample = default_values.copy()\n",
    "        sample[feature_idx] = value\n",
    "        \n",
    "        # Make prediction\n",
    "        sample_scaled = scaler.transform([sample])\n",
    "        pred_proba = model.predict(sample_scaled, verbose=0)\n",
    "        pred_class = np.argmax(pred_proba, axis=1)[0]\n",
    "        \n",
    "        predictions.append(pred_class)\n",
    "    \n",
    "    return feature_range, predictions\n",
    "\n",
    "# Visualize impact of each feature\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "features = ['tds', 'turbidity', 'ph']\n",
    "feature_labels = ['TDS (mg/L)', 'Turbidity (NTU)', 'pH Level']\n",
    "\n",
    "for i, (feature, label) in enumerate(zip(features, feature_labels)):\n",
    "    feature_range, predictions = visualize_feature_impact(feature, i, model, scaler)\n",
    "    \n",
    "    # Create color map for predictions\n",
    "    colors_map = {0: 'red', 1: 'orange', 2: 'lightblue', 3: 'green'}\n",
    "    pred_colors = [colors_map[pred] for pred in predictions]\n",
    "    \n",
    "    axes[i].scatter(feature_range, predictions, c=pred_colors, alpha=0.7, s=20)\n",
    "    axes[i].set_xlabel(label)\n",
    "    axes[i].set_ylabel('Predicted Quality Class')\n",
    "    axes[i].set_title(f'Impact of {label} on Water Quality')\n",
    "    axes[i].set_yticks([0, 1, 2, 3])\n",
    "    axes[i].set_yticklabels(['Poor', 'Acceptable', 'Good', 'Excellent'])\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc38c006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model performance summary\n",
    "def generate_model_summary(model, X_test, y_test, y_pred, history):\n",
    "    \"\"\"Generate comprehensive model performance summary\"\"\"\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"            WATER QUALITY PREDICTION MODEL SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Model architecture info\n",
    "    total_params = model.count_params()\n",
    "    trainable_params = sum([np.prod(layer.get_weights()[0].shape) \n",
    "                           for layer in model.layers if layer.get_weights()])\n",
    "    \n",
    "    print(f\"\\n📊 MODEL ARCHITECTURE:\")\n",
    "    print(f\"   • Total parameters: {total_params:,}\")\n",
    "    print(f\"   • Trainable parameters: {trainable_params:,}\")\n",
    "    print(f\"   • Number of layers: {len(model.layers)}\")\n",
    "    \n",
    "    # Performance metrics\n",
    "    test_loss, test_acc = model.evaluate(X_test, to_categorical(y_test), verbose=0)\n",
    "    \n",
    "    print(f\"\\n🎯 PERFORMANCE METRICS:\")\n",
    "    print(f\"   • Test Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "    print(f\"   • Test Loss: {test_loss:.4f}\")\n",
    "    \n",
    "    # Training info\n",
    "    epochs_trained = len(history.history['loss'])\n",
    "    best_val_acc = max(history.history['val_accuracy'])\n",
    "    \n",
    "    print(f\"\\n🏃 TRAINING INFO:\")\n",
    "    print(f\"   • Epochs trained: {epochs_trained}\")\n",
    "    print(f\"   • Best validation accuracy: {best_val_acc:.4f}\")\n",
    "    \n",
    "    # Quality-specific performance\n",
    "    print(f\"\\n💧 QUALITY-SPECIFIC PERFORMANCE:\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    for i in range(4):\n",
    "        class_acc = cm[i, i] / cm[i, :].sum() if cm[i, :].sum() > 0 else 0\n",
    "        class_samples = cm[i, :].sum()\n",
    "        print(f\"   • {QUALITY_LABELS[i]}: {class_acc:.4f} accuracy ({class_samples} samples)\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "generate_model_summary(model, X_test_scaled, y_test, y_test_pred, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cf4684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive prediction demonstration\n",
    "print(\"🔬 INTERACTIVE WATER QUALITY TESTING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Demo samples representing different scenarios\n",
    "demo_samples = [\n",
    "    (200, 0.5, 7.3, \"🏔️ Mountain spring water\"),\n",
    "    (350, 1.8, 7.1, \"🏠 Filtered tap water\"),\n",
    "    (650, 4.2, 6.9, \"🚰 City tap water\"),\n",
    "    (1100, 8.5, 6.2, \"🌊 Well water\"),\n",
    "    (1800, 18.0, 5.2, \"🏭 Contaminated source\")\n",
    "]\n",
    "\n",
    "results_df = []\n",
    "\n",
    "for tds, turbidity, ph, description in demo_samples:\n",
    "    result = predict_water_quality(tds, turbidity, ph, model, scaler)\n",
    "    \n",
    "    print(f\"\\n{description}\")\n",
    "    print(f\"📈 Sensors: TDS={tds} mg/L, Turbidity={turbidity} NTU, pH={ph}\")\n",
    "    \n",
    "    if \"error\" not in result:\n",
    "        quality_emoji = {'Poor': '🔴', 'Acceptable': '🟡', 'Good': '🔵', 'Excellent': '🟢'}\n",
    "        emoji = quality_emoji[result['quality_label']]\n",
    "        \n",
    "        print(f\"{emoji} Quality: {result['quality_label']} (Confidence: {result['confidence']:.1%})\")\n",
    "        print(f\"💡 {result['recommendation']}\")\n",
    "        \n",
    "        # Store for summary table\n",
    "        results_df.append({\n",
    "            'Description': description.split('�')[1].strip() if '�' in description else description,\n",
    "            'TDS': tds,\n",
    "            'Turbidity': turbidity,\n",
    "            'pH': ph,\n",
    "            'Predicted_Quality': result['quality_label'],\n",
    "            'Confidence': f\"{result['confidence']:.1%}\"\n",
    "        })\n",
    "    else:\n",
    "        print(f\"❌ Error: {result['error']}\")\n",
    "\n",
    "# Create summary table\n",
    "results_summary = pd.DataFrame(results_df)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"📋 PREDICTION SUMMARY TABLE\")\n",
    "print(\"=\"*50)\n",
    "print(results_summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feae1215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model and preprocessor\n",
    "print(\"💾 Saving trained model and preprocessor...\")\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "import os\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Save TensorFlow model\n",
    "model.save('../models/water_quality_model.h5')\n",
    "print(\"✅ Model saved to ../models/water_quality_model.h5\")\n",
    "\n",
    "# Save preprocessor (scaler)\n",
    "import joblib\n",
    "joblib.dump(scaler, '../models/water_quality_model_preprocessor.pkl')\n",
    "print(\"✅ Preprocessor saved to ../models/water_quality_model_preprocessor.pkl\")\n",
    "\n",
    "# Save feature names for future reference\n",
    "feature_info = {\n",
    "    'feature_names': feature_columns,\n",
    "    'quality_labels': QUALITY_LABELS,\n",
    "    'model_version': '1.0',\n",
    "    'training_samples': len(X_train),\n",
    "    'test_accuracy': float(test_accuracy)\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('../models/model_info.json', 'w') as f:\n",
    "    json.dump(feature_info, f, indent=2)\n",
    "print(\"✅ Model info saved to ../models/model_info.json\")\n",
    "\n",
    "print(\"\\n🎉 Model training and saving completed successfully!\")\n",
    "print(\"\\n📝 Next steps:\")\n",
    "print(\"   1. Use the saved model for real-time predictions\")\n",
    "print(\"   2. Deploy the model in a production environment\")\n",
    "print(\"   3. Monitor model performance with new data\")\n",
    "print(\"   4. Retrain periodically with updated datasets\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
